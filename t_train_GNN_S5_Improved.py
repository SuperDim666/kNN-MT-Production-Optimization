# -*- coding: utf-8 -*-
"""T_train_GNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iyYfNA6oQWNHvS1dzla67h52G-6MJgyc

# **GNN Training T**

## **Goal**

1. **Generic:** A *training script* for the dynamics model `T:(S_t,A_t)→S_{t+1}` implemented with a graph neural network (GNN);

2. While retaining the original evaluation indicators (RMSE/Wasserstein-1/Ljung-Box Q/Lyapunov-Positive Rate/Jacobian Spectral Norm), the following evaluation indicators are added:

   * CLF Violation Rate (`clf_violation_rate`)

      * Multi-Step CLF Violation Rate (`nclf_violation_rate`)

   * CBF Violation Rate (`cbf_violation_rate`)

   * Multi-Step Negative Drift Converage (`multi_step_neg_drift_coverage`)

3. The CLF (Controlled Lyapunov) loss is at the core, replacing the "weak Lyapunov penalty";

4. Implement corrections for "defective teacher learning": reduce supervision weights for positive drift/high-stress teacher transfers + CLF loss dominance;

5. The code is complete and runnable (if the local data column name/format is inconsistent with the default, you can configure the mapping through command line parameters).

## **Data Format Description**

Note: *specify the column name prefix through parameters if failed*

Support CSV/NPZ two types of input:

1. ***CSV:*** Expect the following columns to exist (rename configurable):

  * `S_t_*`  (Dimension: `9`)    ---- Current State Vector: default order `[E3, Phi3, H3]`

  * `A_t_*`  (Dimension: `D_A`)  ---- Current Action Vector

  * `S_tp1_*` (Dimension: `9`)   ---- Next State

  * Optional: `seq_id` (Identification of the same sequence for **Ljung-Box Q** and **Multi-Step Evaluation**)

2. ***NPZ:*** Including `S_t_*`, `A_t_*`, `S_tp1_*`, with shapes `[N,9]`, `[N,D_A]`, `[N,9]`; Optional: `seq_id`: `[N]`

## **Command Examples**

```bash
python t_train_GNN.py \
  --train_path data/train.csv --val_path data/val.csv \
  --format csv \
  --epochs 20 --batch_size 512 \
  --adj_mode tri  \
  --rho 0.1 --lambda_clf 1.0 \
  --jacobian_reg 1e-3 \
  --phi_crit 1.1 \
  --rollout_H 5 \
  --action_delta 0.1 --num_delta_dirs 2
```

Dependencies: `Python>=3.8`, `PyTorch>=2.0`, `numpy`, `pandas`

## **Environmental Setup**
"""

import os, sys, locale, argparse
import math, csv, json, random
from typing import Dict, Tuple, List, Optional
from collections import defaultdict
from pathlib import Path
import locale

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler
import scipy.stats as st

# S3 - GNN Framework Upgrade
from torch_geometric.nn import GCNConv, GATConv, RGCNConv

# S5 - Lipschitz Regularization
from torch.nn.utils import spectral_norm

# Hardware Acceleration
from torch import jit
from torch.cuda.amp import GradScaler, autocast

locale.setlocale(locale.LC_ALL, 'en_US.UTF-8')

def to_device(batch, device):
    return {k: v.to(device, non_blocking=True) if torch.is_tensor(v) else v for k, v in batch.items()}

def _set_rand_seed(seed: int):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

    # Enable scalar capture for GNN model
    torch._dynamo.config.capture_scalar_outputs = True

    # Allow CuDNN to automatically find the fastest convolution algorithm.
    # May sacrifice some strict reproducibility across hardware or software versions.
    # 1. Enable during parameter tuning and final training;
    # 2. Disable when strictly reproducing the results in the paper is required.
    # torch.backends.cudnn.deterministic = False
    # torch.backends.cudnn.benchmark = True

"""## **Tool functions: *Numerical*, *Measurement*, *Stability Analysis***"""

# -----------------------------
# Tool functions: numerical, measurement, stability analysis
# -----------------------------
def build_action_candidates(
        S_t: torch.Tensor, A_t: torch.Tensor,
        mode: str, num_delta_dirs: int, action_delta: float
    ) -> torch.Tensor:
    """
    Constructs a candidate action set based on mode, returning a shape of [K, B, dA].
    - S_t: [B, dS] (only used for device retrieval)
    - A_t: [B, dA] (teacher action) (normalized over the domain [-1, 1]), required for `perturb` mode
    - mode in {"onehot","perturb","both"}
    """
    device = S_t.device
    B, dA = A_t.shape

    cands = []

    if mode in ("onehot", "both"):
        # Discrete one-hot candidate: shape [dA, B, dA]
        onehot = torch.eye(dA, device=device).unsqueeze(1).repeat(1, B, 1)
        cands.append(onehot)

    if mode in ("perturb", "both"):
        # Teacher action ± perturbation:
        # Each batch sample makes K_perturb candidates around its A_t
        dirs = min(max(0, num_delta_dirs), dA)
        if dirs > 0:
            eye = torch.eye(dA, device=device)  # [dA, dA]
            # In every direction +/-
            cand_list = []
            for i in range(dirs):
                delta_vec = action_delta * eye[i].view(1, -1)  # [1, dA]
                a_plus = torch.clamp(A_t + delta_vec, -1.0, 1.0)  # [B, dA]
                a_minus = torch.clamp(A_t - delta_vec, -1.0, 1.0)
                cand_list.append(a_plus.unsqueeze(0))   # [1, B, dA]
                cand_list.append(a_minus.unsqueeze(0))  # [1, B, dA]
            if len(cand_list) > 0:
                perturb = torch.cat(cand_list, dim=0)  # [Kp, B, dA]
                cands.append(perturb)
        else:
            # If dirs=0, only teacher action itself will be included in the candidate
            cands.append(A_t.unsqueeze(0))  # [1, B, dA]

    assert len(cands) > 0, "build_action_candidates(): candidate set is empty, check mode/parameters"
    A_cands = torch.cat(cands, dim=0)  # [K_total, B, dA]

    # Deduplication: per-batch execution
    uniq_cands = []
    for b in range(B):
        cand_b = A_cands[:, b, :]  # [K, dA]
        uniq_b = torch.unique(cand_b, dim=0, return_inverse=False) # [K_uniq <=K]

        # Fill back to the original K:
        # If K_uniq < K, randomly repeat the elements in uniq_b
        if uniq_b.shape[0] < A_cands.shape[0]:
            pad_idx = torch.randint(0, uniq_b.shape[0], (A_cands.shape[0] - uniq_b.shape[0],), device=device)
            pad = uniq_b[pad_idx]
            uniq_b = torch.cat([uniq_b, pad], dim=0)
        uniq_cands.append(uniq_b.unsqueeze(1))  # [K, 1, dA]

    A_cands = torch.cat(uniq_cands, dim=1)  # [K, B, dA]

    return A_cands.contiguous()

def discrete_action_candidates(
        action_dim: int, batch_size: int, device: torch.device
    ) -> torch.Tensor:
    """
    Returns discrete action candidates of [K,B,dA],
    where K=action_dim, & each candidate is a standard basis one-hot.
    """
    I = torch.eye(action_dim, device=device)              # [dA,dA]
    A_cands = I.unsqueeze(1).expand(action_dim, batch_size, action_dim)  # [K,B,dA]
    return A_cands

def rmse(pred: torch.Tensor, tgt: torch.Tensor) -> float:
    return torch.sqrt(torch.mean((pred - tgt) ** 2)).item()

def wasserstein1_vector(pred: torch.Tensor, tgt: torch.Tensor) -> float:
    """
    Approximate vector W1:
    1. calculate L1 in each dimension and average;
    2. sufficiently stable for engineering purposes.

    Note: strict multidimensional EMD relies on discrete distribution
    """
    return torch.mean(torch.abs(pred - tgt)).item()

def ljung_box_Q(residuals: np.ndarray, lags: int = 10) -> float:
    """
    Box-Ljung Q statistic.
    Requires residuals to be a time-ordered one-dimensional sequence.

    If the data is not a sequence or is not long enough, NaN is returned.
    """
    r = np.asarray(residuals).reshape(-1)
    n = len(r)
    if n < lags + 2:
        return float("nan")
    r = r - r.mean()
    acfs = []
    denom = np.sum(r ** 2)
    if denom <= 1e-12:
        return 0.0
    for k in range(1, lags + 1):
        acf = np.sum(r[k:] * r[:-k]) / denom
        acfs.append(acf)
    Q = n * (n + 2) * np.sum([(acfs[k - 1] ** 2) / (n - k) for k in range(1, lags + 1)])
    return float(Q)

def spectral_norm_jacobian_mean(
        model, S_t: torch.Tensor, A_t: torch.Tensor,
        num_power_iter: int = 10, sample_size: int = 64,
        edge_index: Optional[torch.Tensor] = None,
        edge_type: Optional[torch.Tensor] = None
    ) -> float:
    """
    Estimate the mean of the spectral norm of dT/dS: random batch sampling.
    Use power iteration: largest singular value ≈ ||J v|| / ||v||.
    """
    B = S_t.shape[0]
    idx = torch.randperm(B, device=S_t.device)[:min(sample_size, B)]
    S = S_t[idx].detach().clone().requires_grad_(True)
    A = A_t[idx].detach().clone()

    with torch.enable_grad():
        def Jv_func(v):
            out, _ = model(S, A, edge_index, edge_type)  # [b, 9]
            (Jv,) = torch.autograd.grad(
                outputs=out,
                inputs=S,
                grad_outputs=v,
                retain_graph=True,
                create_graph=True,
                allow_unused=False,
            )
            return Jv

        # Initialize v
        v = torch.randn_like(S)
        v = v / (torch.norm(v, dim=1, keepdim=True) + 1e-12)
        for _ in range(num_power_iter):
            Jv = Jv_func(v)
            sigma = torch.norm(Jv, dim=1, keepdim=True) + 1e-12
            v = Jv / sigma

        # Final estimated singular values
        Jv = Jv_func(v)
        sigma = torch.norm(Jv, dim=1) / (torch.norm(v, dim=1) + 1e-12)
        return sigma.mean().detach().item()

def build_adjacency(
        mode: str = "tri",
        num_relations: int = 5,
        mapping_strategy: str = "auto"
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    [S4 Revision] Constructs a 9x9 graph structure.
    Returns:
    - adj: [9,9] dense adjacency matrix (for manual GCN implementation)
    - edge_index: [2, num_edges] PyG standard format
    - edge_type: [num_edges] relation type for RGCN
    """
    num_nodes = 9
    nodes = np.arange(num_nodes)
    adj = np.zeros((num_nodes, num_nodes), dtype=np.float32)
    edge_list = []
    edge_type_list = []

    # Relationship type definition:
    # 0=self-loop, 1=internal_of_E, 2=internal_of_Phi, 3=internal_of_H, 4=cross-group
    node_groups = {0:0, 1:0, 2:0, 3:1, 4:1, 5:1, 6:2, 7:2, 8:2} # 0:E, 1:Phi, 2:H

    for i in range(num_nodes):
        # Add self-loop
        edge_list.append((i, i))
        edge_type_list.append(0)
        adj[i, i] = 1.0

    if mode == "full":
        for i in range(num_nodes):
            for j in range(i + 1, num_nodes):
                edge_list.extend([(i, j), (j, i)])
                adj[i, j] = adj[j, i] = 1.0
                # Determine the relationship type
                rel_type = node_groups[i] + 1 if node_groups[i] == node_groups[j] else 4
                edge_type_list.extend([rel_type, rel_type])

    elif mode == "tri":
        # Full connection within the group
        blocks = [range(0, 3), range(3, 6), range(6, 9)]
        for group_idx, block in enumerate(blocks):
            for i in block:
                for j in block:
                    if i != j:
                        edge_list.append((i, j))
                        edge_type_list.append(group_idx + 1)
                        adj[i, j] = 1.0
        # Full connection within the group
        for i in range(num_nodes):
            for j in range(i + 1, num_nodes):
                if node_groups[i] != node_groups[j]:
                    edge_list.append((i, j))
                    edge_list.append((j, i))
                    edge_type_list.extend([4, 4])
                    adj[i, j] = adj[j, i] = 1.0

    edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()
    edge_type = torch.tensor(edge_type_list, dtype=torch.long)

    # Dynamic Relationship Type Mapping
    if num_relations < 5:
        mapping = torch.zeros(5, dtype=torch.long)

        if mapping_strategy == "auto":
            # Default Smart Mapping
            if num_relations == 1:
                mapping[:] = 0
            elif num_relations == 2:
                mapping[0] = 0  # self-loop
                mapping[1:5] = 1  # fully connected
            elif num_relations == 3:
                mapping[0] = 0  # self-loop
                mapping[1:4] = 1  # in-group
                mapping[4] = 2    # cross-group
            elif num_relations == 4:
                mapping[0] = 0  # self-loop
                mapping[1] = 1  # inner-E
                mapping[2] = 2  # inner-Phi
                mapping[3] = 3  # inner-H
                mapping[4] = 3  # cross-group -> inner-H

        elif mapping_strategy == "group_internal":
            # In-group priority mapping strategy
            if num_relations == 1:
                mapping[:] = 0
            elif num_relations == 2:
                mapping[0] = 0  # self-loop
                mapping[1:4] = 1  # inner-group
                mapping[4] = 1    # merge cross-group into the inner-group
            elif num_relations == 3:
                mapping[0] = 0  # self-loop
                mapping[1] = 1  # E
                mapping[2] = 2  # Phi
                mapping[3] = 1  # H -> E
                mapping[4] = 2  # cross-group -> Phi
            elif num_relations == 4:
                mapping[0] = 0  # self-loop
                mapping[1] = 1  # E
                mapping[2] = 2  # Phi
                mapping[3] = 3  # H
                mapping[4] = 3  # cross-group -> H

        elif mapping_strategy == "functional":
            # Function-oriented mapping strategy
            if num_relations == 1:
                mapping[:] = 0
            elif num_relations == 2:
                mapping[0] = 0  # self-loop
                mapping[1] = 1  # E-related
                mapping[2:5] = 1  # others
            elif num_relations == 3:
                mapping[0] = 0  # self-loop
                mapping[1] = 1  # E
                mapping[2] = 2  # Phi
                mapping[3:5] = 2  # H
            elif num_relations == 4:
                mapping[0] = 0  # self-loop
                mapping[1] = 1  # E
                mapping[2] = 2  # Phi
                mapping[3] = 3  # H
                mapping[4] = 3  # cross-group -> H

        # Applying Mappings
        edge_type = mapping[edge_type]

    return torch.from_numpy(adj), edge_index, edge_type

def split_state(S: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    Decompose 9-dimensional S into E(0:3), Phi(3:6), H(6:9)
    """
    E = S[:, 0:3]
    Phi = S[:, 3:6]
    H = S[:, 6:9]
    return E, Phi, H

def lyapunov_V(E: torch.Tensor, P_diag: torch.Tensor) -> torch.Tensor:
    """
    V(E) = E^T P E, P is diagonally positive (use positive diagonals).
    - E: [B,3]
    - P_diag: [3] (positive), maintained positive by softplus
    Return V: [B]
    """
    # (E * P_diag)·E
    return torch.sum((E * P_diag.view(1, 3)) * E, dim=1)

def clf_loss(
        model, S_t: torch.Tensor, A_t: torch.Tensor,
        V_fn, rho: float,
        edge_index: Optional[torch.Tensor] = None, edge_type: Optional[torch.Tensor] = None,
        action_search: str = "onehot", num_delta_dirs: int = 2, action_delta: float = 0.1,
        softmin_tau: float = 0.5,
        # S5: CBF-related params
        lambda_cbf: float = 0.0,
        phi_crit: float = 0.0,
        cbf_alpha: float = 0.1
    ) -> Tuple[torch.Tensor, float]:
    """
    CLF loss: For each example, a set of candidate actions is sampled, including teacher's A_t and its perturbation.
    Then a softmin approximation `min_a [V(S_{t+1}(a)) - (1-rho)*V(S_t)]` is applied.
    Then a `ReLU(·)` is used as the violation rate, and the batch mean is the CLF loss.
    Returns: (loss_tensor, violation_rate)

    [S5 Revision] CLF loss:
    1. Adapts the new output interface of the model (S_pred, log_risk).
    2. Integrates the CBF soft constraint to make it effective in single-step evaluation and training.
    """
    B, dA = A_t.shape
    device = S_t.device
    # Action candidates: Center (teacher) + Interference (± along each direction)
    cand_list = [A_t]
    # Control the number of perturbation directions:
    # - select the first `num_delta_dirs` directions (or all) from the standard basis vectors
    dirs = min(num_delta_dirs, dA)
    eye = torch.eye(dA, device=device)
    for i in range(dirs):
        delta = action_delta * eye[i].view(1, -1)
        cand_list.append(torch.clamp(A_t + delta, -1.0, 1.0))
        cand_list.append(torch.clamp(A_t - delta, -1.0, 1.0))

    A_cands = build_action_candidates(
        S_t=S_t,
        A_t=A_t,  # Teacher Actions (Standardized Domains)
        mode= action_search,
        num_delta_dirs=num_delta_dirs,
        action_delta=action_delta
    )  # [K, B, dA]
    K = A_cands.shape[0]

    with torch.enable_grad():
        # Gradients have been ensured to flow to the parameters upstream
        # - no derivatives are taken for S_t
        S_t_detached = S_t
        # Calculate the next state and ΔV under each candidate action
        V_t = V_fn(S_t_detached)  # [B]
        deltaV = []
        all_S_next_k = [] # For subsequent calculation of CBF
        for k in range(K):
            S_tp1_k, _ = model(S_t_detached, A_cands[k], edge_index, edge_type) # [B,9]
            all_S_next_k.append(S_tp1_k)

            E_tp1_k, _, _ = split_state(S_tp1_k)
            V_tp1_k = V_fn_from_E = V_fn(S_tp1_k) if V_fn.__name__ != "_v_from_E" else V_fn(S_tp1_k)
            V_tp1_k = V_fn_from_E if isinstance(V_fn_from_E, torch.Tensor) else None
            if V_tp1_k is None:
                V_tp1_k = V_fn(E_tp1_k)

            deltaV_k = V_tp1_k - (1.0 - rho) * V_t  # [B]

            deltaV.append(deltaV_k)
        # [K,B] → softmin approximates min
        deltaV_stack = torch.stack(deltaV, dim=0)  # [K,B]
        all_S_next_k_stack = torch.stack(all_S_next_k, dim=0) # [K, B, dS]

        # softmin: -tau * logsumexp(-x/tau)
        # 1. softmin not only gives an approximate minimum ΔV,
        # 2. its weight can also be used to calculate the weighted average S_next
        softmin_weights = F.softmax(-deltaV_stack.t() / softmin_tau, dim=-1) # [B, K]
        softmin_vals = torch.einsum("bk,kb->b", softmin_weights, deltaV_stack) # [B]
        violations = F.relu(softmin_vals)  # Violation amount (>0 means violation)
        loss = violations.mean()
        violation_rate = (violations > 0).float().mean().item()

        # --- S5 (CBF): CBF Loss ---
        if lambda_cbf > 0 and phi_crit > 0:
            # Ensure that single-step evaluation and training also reflect CBF constraints to maintain consistency with the n_step version.
            # We impose a CBF constraint on the expected next state obtained by softmin weighting.
            best_S_next_soft = torch.einsum("bk,kbj->bj", softmin_weights, all_S_next_k_stack)
            cbf_l = control_barrier_loss(S_t_detached, best_S_next_soft, phi_crit, cbf_alpha)
            loss += lambda_cbf * cbf_l

        return loss, violation_rate

def cvar_loss(violations: torch.Tensor, alpha: float) -> torch.Tensor:
    # violations: >=0 violation amount (ReLU has been done)
    n = violations.numel()
    if n == 0:
        return violations.new_tensor(0.0)
    tail = max(int(math.ceil((1.0 - alpha) * n)), 1)
    # Only perform CVaR on positive violations
    # Ensure non-zero violations upstream
    topk, _ = torch.topk(violations, tail, largest=True, sorted=False)
    return topk.mean()

# --- S5: Cox Loss Function ---
@jit.script
def cox_ph_loss(log_risks: torch.Tensor, times: torch.Tensor, events: torch.Tensor) -> torch.Tensor:
    """
    Computes the negative partial log-likelihood loss for the Cox proportional hazards model.

    - Penalizes the model for failing to assign a higher risk to samples with "early events" than to samples with "late events/no events,"
      directly guiding the model to learn to avoid the "coherence horizon."
    """
    # Sort by event time in descending order, which is the standard way to calculate risk sets
    times, sort_indices = torch.sort(times, descending=True)
    events = events[sort_indices]
    log_risks = log_risks[sort_indices]

    # Compute the log sum of the risk at each time point (logsumexp is numerically stable)
    log_risk_sum = torch.logsumexp(log_risks, dim=0)

    # Only calculate the loss for samples where events occur (events == 1)
    loss = -torch.sum((log_risks - log_risk_sum)[events == 1])

    # Returns the batch average loss
    num_events = torch.sum(events)
    return loss / num_events if num_events > 0 else torch.tensor(0.0, device=log_risks.device)

# --- S5: CBF Soft-Constraint Loss ---
@jit.script
def control_barrier_loss(S_t: torch.Tensor, S_tp1: torch.Tensor, phi_crit: float, cbf_alpha: float) -> torch.Tensor:
    """
    Calculates the CBF Soft-Constraint Loss.

    - When the predicted next state S_tp1 would bring the system closer to or across the safety boundary (defined by phi_crit),
      a penalty is imposed to enforce the safety of the system.
    """
    _, Phi_t, _ = split_state(S_t)
    _, Phi_tp1, _ = split_state(S_tp1)

    # Barrier function h(S) = (phi_crit^2 - ||Phi||^2) >= 0
    h_t = phi_crit**2 - torch.sum(Phi_t**2, dim=1)
    h_tp1 = phi_crit**2 - torch.sum(Phi_tp1**2, dim=1)

    # Discrete CBF conditions: h(S_{t+1}) >= (1 - cbf_alpha) * h(S_t)
    # Violation = Relu(-(h_tp1 - (1 - cbf_alpha) * h_t))
    violations = F.relu(-(h_tp1 - (1.0 - cbf_alpha) * h_t))
    return violations.mean()

def n_step_clf_loss(
    model: nn.Module,
    S_t: torch.Tensor,
    A_t: torch.Tensor,
    V_fn: callable,
    # --- S3 params ---
    nstep_H: int,
    nstep_gamma: float,
    nstep_selector: str,
    nstep_bptt_window: int,
    use_cvar_loss: bool,
    cvar_alpha: float,
    use_epsilon_greedy: bool,
    epsilon: float,
    policy_entropy_weight: float,
    # --- S1 params ---
    rho: float,
    action_search: str,
    num_delta_dirs: int,
    action_delta: float,
    gumbel_tau: float, # Gumbel-Softmax Temperature, passed in by the scheduler
    # --- S4 params for GNN-based framework upgrade (GATConv/RGCNConv) ---
    edge_index, edge_type,
    # --- S5 params ---
    lambda_adt: float,
    lambda_cbf: float,
    phi_crit: float,
    cbf_alpha: float,
    # --- S3: eval-mode params ---
    return_dv_trajectory: bool = False
) -> Tuple[torch.Tensor, dict]:
    """
    [S3 Core Implementation] Calculates N-step CLF loss, integrating multiple advanced mechanisms.
    [S5 Revision] Integrates ADT and CBF losses based on S3/S4.
    """
    B, dS = S_t.shape
    total_loss = 0.0
    action_indices_hist = []

    # For evaluation mode
    dv_trajectory = [] if return_dv_trajectory else None

    S_current = S_t
    # Reference: Report 3 of S2 - Rolling Update of Candidate Anchors
    A_anchor = A_t

    # Reference: Report 3 of S2 - BPTT Window
    history_S = [S_current]

    # The switching rate at each step is collected to be returned in evaluation mode.
    adt_switch_rates = []

    # S5 (ADT): Record previous step
    # - index of initial action (teacher) is used as starting point of ADT calculation.
    # - Ensure that switch from step 0 to 1 is also penalized.
    with torch.no_grad():
        prev_indices = torch.argmax(A_t, dim=1) # [B]

    for i in range(nstep_H):
        # 1. Generate candidate actions
        cand_actions = build_action_candidates(
            S_t=S_current, A_t=A_anchor,
            mode=action_search,
            num_delta_dirs=num_delta_dirs,
            action_delta=action_delta
        ) # Shape: [K, B, dA]
        K = cand_actions.shape[0]

        # 2. Calculate ΔV for all candidate actions
        with torch.no_grad():
            V_current_no_grad = V_fn(S_current)

        all_S_next_k = []
        all_dV_k = []
        for k in range(K):

            S_next_k, _ = model(S_current, cand_actions[k], edge_index, edge_type)

            V_next_k = V_fn(S_next_k)
            dV_k = V_next_k - (1.0 - rho) * V_current_no_grad
            all_S_next_k.append(S_next_k)
            all_dV_k.append(dV_k)

        all_S_next_k_stack = torch.stack(all_S_next_k, dim=0) # [K, B, dS]
        all_dV_k_stack = torch.stack(all_dV_k, dim=0)       # [K, B]

        # 3. Action Selection (Selector)
        # Reference: Report 4 of S2 - Selector Annealing and Throughput Estimation
        if nstep_selector == 'softmin':
            weights = F.softmax(-all_dV_k_stack.t() / gumbel_tau, dim=-1) # [B, K]
            best_k_indices_no_grad = torch.argmin(all_dV_k_stack, dim=0)
        elif nstep_selector == 'gumbel_st':
            y_soft = F.gumbel_softmax(logits=-all_dV_k_stack.t(), tau=gumbel_tau, hard=False, dim=-1)
            y_hard = F.gumbel_softmax(logits=-all_dV_k_stack.t(), tau=gumbel_tau, hard=True, dim=-1)
            weights = y_hard - y_soft.detach() + y_soft # Straight-Through Trick
            best_k_indices_no_grad = torch.argmax(weights, dim=-1)
        else: # 'hard_greedy'
            best_k_indices = torch.argmin(all_dV_k_stack, dim=0)
            weights = F.one_hot(best_k_indices, num_classes=K).float() # [B, K]
            best_k_indices_no_grad = best_k_indices

        # Reference: Report 4 of S2 - Candidate Diversity and Exploration (ε-greedy)
        if use_epsilon_greedy and model.training:
            random_indices = torch.randint(0, K, (B,), device=S_t.device)
            use_random = (torch.rand(B, device=S_t.device) < epsilon)
            final_indices = torch.where(use_random, random_indices, best_k_indices_no_grad)
            weights = F.one_hot(final_indices, num_classes=K).float()
        else:
            final_indices = best_k_indices_no_grad

        action_indices_hist.append(final_indices.detach().cpu().numpy())

        # 4. Collect the loss and next state of the current step
        best_dV_this_step = torch.einsum("bk,kb->b", weights, all_dV_k_stack)
        best_S_next = torch.einsum("bk,kbj->bj", weights, all_S_next_k_stack)

        if return_dv_trajectory:
            dv_trajectory.append(best_dV_this_step.detach())

        # 5. Accumulated losses
        violations = F.relu(best_dV_this_step)

        # Reference: Report 4 of S2 - Quantile/Critical Value at Risk (CVaR)
        step_loss = cvar_loss(violations.float(), cvar_alpha) if use_cvar_loss else violations.mean()

        # S5 (CBF): CBF Loss
        if lambda_cbf > 0 and phi_crit > 0:
            cbf_l = control_barrier_loss(S_current, best_S_next, phi_crit, cbf_alpha)
            step_loss += lambda_cbf * cbf_l

        # S5 (ADT): ADT Loss
        # - Penalizing action switching between consecutive time steps encourages the generation of smoother and more stable policies.
        if lambda_adt > 0 or not model.training:
            if prev_indices is not None:
                switch_indicators = (final_indices != prev_indices).float()
                adt_l = switch_indicators.mean()
                # Regardless of whether ADT loss is enabled or not,
                # the handover rate metric is collected at evaluation time
                if not model.training:
                    adt_switch_rates.append(adt_l.item())
                if lambda_adt > 0:
                    step_loss += lambda_adt * adt_l

        total_loss += (nstep_gamma ** i) * step_loss

        # S5 (ADT): Update the previous action to prepare for the next iteration
        prev_indices = final_indices.detach()

        # 6. Prepare for the next iteration
        # Reference: Report 3 of S2 - BPTT window adaption
        S_current = best_S_next
        history_S.append(S_current)
        if nstep_bptt_window > 0 and len(history_S) > nstep_bptt_window:
            history_S[0] = history_S[0].detach()
            history_S.pop(0)

        A_anchor = cand_actions[final_indices, torch.arange(B)]

    # 7. Policy Entropy Regularization
    if policy_entropy_weight > 0 and len(action_indices_hist) > 0:
        all_actions = np.concatenate(action_indices_hist)
        counts = np.bincount(all_actions, minlength=K)
        probs = counts / counts.sum()
        probs = torch.from_numpy(probs).float().to(S_t.device)
        entropy = -torch.sum(probs * torch.log(probs + 1e-9))
        total_loss -= policy_entropy_weight * entropy

    # Constructing diagnostic statistics
    stats = {
        f"clf_violation_rate_step{i}": (all_dV_k_stack[action_indices_hist[i], torch.arange(B)] > 0).float().mean().item()
        for i in range(nstep_H)
    }
    stats["clf_violation_rate_mean"] = np.mean(list(stats.values()))

    # Add the collected average toggle rates to the returned statistics dictionary.
    if adt_switch_rates:
        stats["adt_switch_rate"] = np.mean(adt_switch_rates)

    if return_dv_trajectory:
        stats["dv_trajectory"] = torch.stack(dv_trajectory, dim=1) # [B, H]

    return total_loss, stats

def lyapunov_positive_rate(
        model, S_t: torch.Tensor, A_t: torch.Tensor, V_fn,
        edge_index: Optional[torch.Tensor] = None, edge_type: Optional[torch.Tensor] = None
    ) -> float:
    with torch.no_grad():
        V_t = V_fn(S_t)
        S_tp1, _ = model(S_t, A_t, edge_index, edge_type)
        E_tp1, _, _ = split_state(S_tp1)
        # If V_fn receives S, it is uniformly passed from S;
        # otherwise it is calculated from E
        V_tp1 = V_fn(S_tp1) if V_fn.__name__ != "_v_from_E" else V_fn(E_tp1)
        dv = V_tp1 - V_t
        return (dv > 0).float().mean().item()

def multi_step_negative_drift_coverage(
        model, S_t: torch.Tensor, A_t: torch.Tensor, V_fn,
        rho: float, rollout_H: int = 5,
        action_search: str = "onehot", action_delta: float = 0.1, num_delta_dirs: int = 2,
        softmin_tau: float = 0.5,
        edge_index: Optional[torch.Tensor] = None, edge_type: Optional[torch.Tensor] = None
    ) -> float:
    """
    Perform multi-step rolling based on the "existence of stable actions" perspective:
    At each step, select the action that minimizes ΔV from the candidate action set using softmin,
    and count the percentage of steps where ΔV <= 0.

    Returns: Coverage (0-1)
    """
    B, dA = A_t.shape
    device = S_t.device
    S = S_t.clone()
    total, neg_count = 0, 0
    eye = torch.eye(dA, device=device)

    for _ in range(rollout_H):
        # Use switches to construct candidate actions uniformly:
        # - If mode=perturb/both, perform ± perturbations around the "teacher action A_t";
        # - If mode=onehot, it has nothing to do with A_t and directly returns the one-hot candidate.
        A_cands = build_action_candidates(
            S_t=S,      # Use the current scroll state
            A_t=A_t,    # Pass in teacher action A_t for current batch (the A_teacher variable is not needed here)
            mode=action_search,
            num_delta_dirs=num_delta_dirs,
            action_delta=action_delta
        )  # [K, B, dA]
        K = A_cands.shape[0]

        with torch.no_grad():
            V_now = V_fn(S)
            best_dv = None
            best_S_next = None
            for k in range(K):
                S_next_k, _ = model(S, A_cands[k], edge_index, edge_type)
                V_next_k = V_fn(S_next_k) if V_fn.__name__ != "_v_from_E" else V_fn(split_state(S_next_k)[0])
                dv_k = V_next_k - (1.0 - rho) * V_now  # Corresponding CLF format
                if best_dv is None:
                    best_dv, best_S_next = dv_k, S_next_k
                else:
                    mask = dv_k < best_dv
                    best_dv = torch.where(mask, dv_k, best_dv)
                    best_S_next = torch.where(mask.view(-1, 1), S_next_k, best_S_next)
            neg_count += (best_dv <= 0).float().sum().item()
            total += B
            S = best_S_next

    if total == 0:
        return float("nan")
    return float(neg_count / total)

def multi_step_negative_drift_coverage_teacher(
        model, S_t: torch.Tensor, A_t: torch.Tensor,
        V_fn, rho: float,
        edge_index: Optional[torch.Tensor] = None, edge_type: Optional[torch.Tensor] = None,
        rollout_H: int = 5
    ) -> float:
    """
    Comparison: Use only "Teacher Action" to scroll, without searching for candidate actions.
    """
    with torch.no_grad():
        S = S_t.clone()
        total, neg_count = 0, 0
        for _ in range(rollout_H):
            V_now = V_fn(S)
            S_next, _ = model(S, A_t, edge_index, edge_type)
            V_next = V_fn(S_next)
            dv = V_next - (1.0 - rho) * V_now
            neg_count += (dv <= 0).float().sum().item()
            total += S.shape[0]
            S = S_next
        if total == 0:
            return float("nan")
        return float(neg_count / total)


def multi_step_negative_drift_coverage_fixed(
        model, S_t: torch.Tensor,
        V_fn, rho: float,
        edge_index: Optional[torch.Tensor] = None, edge_type: Optional[torch.Tensor] = None,
        rollout_H: int = 5,
        fixed_idx: int = 0, action_dim: int = 5
    ) -> float:
    """
    Comparison: Roll using fixed one-hot action e_{fixed_idx}.
    """
    with torch.no_grad():
        B = S_t.shape[0]
        device = S_t.device
        A_fixed = torch.zeros(B, action_dim, device=device)
        A_fixed[:, int(fixed_idx)] = 1.0

        S = S_t.clone()
        total, neg_count = 0, 0
        for _ in range(rollout_H):
            V_now = V_fn(S)
            S_next, _ = model(S, A_fixed, edge_index, edge_type)
            V_next = V_fn(S_next)
            dv = V_next - (1.0 - rho) * V_now
            neg_count += (dv <= 0).float().sum().item()
            total += B
            S = S_next
        if total == 0:
            return float("nan")
        return float(neg_count / total)

"""## **Dataset**"""

# -----------------------------
# Dataset
# -----------------------------

# --- CSV preprocessing required columns ---
STATE_COLS_DEFAULT = [
    'error_semantic','error_fluency','error_faithfulness',
    'pressure_latency','pressure_memory','pressure_throughput',
    'context_uncertainty','context_confidence','context_relevance'
]

def _build_pairs_from_df(df: pd.DataFrame, action_dim: int=5, event_threshold: float = 2.0):
    # Sort + shift(-1) within the group to generate next_
    need_cols = set(STATE_COLS_DEFAULT + ['Strategy','sample_id','step','action_idx'])
    missing = [c for c in need_cols if c not in df.columns]
    if missing:
        raise ValueError(f"CSV is missing a required column: {missing}")

    # Sorting ['Strategy','sample_id','step']
    df = df.sort_values(['Strategy','sample_id','step'])

    # Generate group_id (unique for each (Strategy, sample_id))
    group_keys = df[['Strategy','sample_id']].apply(tuple, axis=1)
    seq_id_full, _ = pd.factorize(group_keys)  # [N]

    # S5 (Cox): Make sure error_norm exists to define the event
    if 'error_norm' not in df.columns:
        df['error_norm'] = np.linalg.norm(df[['error_semantic','error_fluency','error_faithfulness']].values, axis=1)

    # Shift(-1) group by group to generate next_
    for c in STATE_COLS_DEFAULT + ['error_norm']:
        df[f'next_{c}'] = df.groupby(['Strategy','sample_id'])[c].shift(-1)

    # Valid rows (with next step)
    valid = ~pd.isna(df[[f'next_{c}' for c in STATE_COLS_DEFAULT]]).any(axis=1)
    df = df.loc[valid].reset_index(drop=True)
    seq_id = seq_id_full[valid.values]  # align with sample pairs

    S_t   = df[STATE_COLS_DEFAULT].astype(np.float32).values
    S_tp1 = df[[f'next_{c}' for c in STATE_COLS_DEFAULT]].astype(np.float32).values
    a_idx = df['action_idx'].astype(int).values
    A = np.zeros((len(a_idx), action_dim), dtype=np.float32)
    A[np.arange(len(a_idx)), np.clip(a_idx, 0, action_dim-1)] = 1.0

    # --- S5 (Cox): Generate event times and indications ---
    # Determine for each trajectory whether and when its “failure event” (coherence horizon) occurs.
    # Implementation of the data basis for L_cox.
    df['event_occurred'] = df['error_norm'] > event_threshold

    df['group_id'] = seq_id
    first_event_times = df[df['event_occurred']].groupby('group_id')['step'].min()

    df = df.merge(first_event_times.rename('first_event_step'), on='group_id', how='left')

    max_steps = df.groupby('group_id')['step'].max()
    df = df.merge(max_steps.rename('max_step'), on='group_id', how='left')

    # T: The time of the first event in the trajectory. If it never happened, it is the last step of the trajectory (censoring time).
    # E: 1 if T is the true event time, 0 if it is the censored time.
    df['T'] = df['first_event_step'].fillna(df['max_step'])
    df['E'] = (~df['first_event_step'].isna()).astype(np.int64)

    T = df['T'].astype(np.float32).values
    E = df['E'].astype(np.int64).values

    return S_t, A, S_tp1, seq_id.astype(np.int64), T, E

class ArrayTripletDataset(Dataset):
    def __init__(
            self,
            S_t, A, S_tp1, scaler: StandardScaler,
            T: Optional[np.ndarray], E: Optional[np.ndarray],
            seq_id: Optional[np.ndarray]=None
        ):
        self.S_t   = torch.from_numpy(scaler.transform(S_t).astype(np.float32))
        self.S_tp1 = torch.from_numpy(scaler.transform(S_tp1).astype(np.float32))
        self.A_t   = torch.from_numpy(A.astype(np.float32))
        self.seq_id = torch.from_numpy(seq_id) if seq_id is not None else None
        # S5 (Cox)
        self.T = torch.from_numpy(T)
        self.E = torch.from_numpy(E)
    def __len__(self): return self.S_t.shape[0]
    def __getitem__(self, i: int):
        item = {"S_t": self.S_t[i], "A_t": self.A_t[i], "S_tp1": self.S_tp1[i]}
        if self.seq_id is not None: item["seq_id"] = self.seq_id[i]
        if self.T is not None: item["T"] = self.T[i]
        if self.E is not None: item["E"] = self.E[i]
        return item

def build_pairs_from_csv(
        df: pd.DataFrame,
        state_cols: List[str] = STATE_COLS_DEFAULT,
        group_cols=('Strategy','sample_id','step'),
        action_col: str = 'action_idx',
        action_dim: int = 5
    ) -> Tuple[np.ndarray,np.ndarray,np.ndarray]:
    """
    Same as FNN: sort by (Strategy, sample_id, step), shift (-1) within the group to generate the next_ column, and discard trailing NaNs.
    Returns (S_t, A_onehot, S_tp1)
    """
    assert all(c in df.columns for c in state_cols), "Status column is missing; please check your CSV and column names"
    assert action_col in df.columns, "Missing action column action_idx"

    # Sort + shift(-1) within the group to generate next_
    df = df.sort_values(list(group_cols))
    for c in state_cols:
        df[f'next_{c}'] = df.groupby(['Strategy','sample_id'])[c].shift(-1)

    # Discard rows without next step
    valid = ~pd.isna(df[[f'next_{c}' for c in state_cols]]).any(axis=1)
    df = df.loc[valid].reset_index(drop=True)

    S_t  = df[state_cols].astype(np.float32).values
    S_tp1 = df[[f'next_{c}' for c in state_cols]].astype(np.float32).values

    # action_idx -> one-hot
    a_idx = df[action_col].astype(int).values
    A = np.zeros((len(a_idx), action_dim), dtype=np.float32)
    A[np.arange(len(a_idx)), np.clip(a_idx, 0, action_dim-1)] = 1.0
    return S_t, A, S_tp1

class FNNAlignedDataset(Dataset):
    """
    Convert the FNN's CSV convention to (S_t, A_t_onehot, S_tp1);
    Apply the StandardScaler consistent with the training set.
    """
    def __init__(
            self,
            csv_path: str, scaler: StandardScaler,
            state_cols: List[str] = STATE_COLS_DEFAULT,
            action_dim: int = 5
        ):
        super().__init__()
        df = pd.read_csv(csv_path)
        S_t, A, S_tp1 = build_pairs_from_csv(df, state_cols=state_cols, action_dim=action_dim)
        # Same as FNN Implementation: only normalize the state (scaler fitted by the training set)
        self.S_t   = torch.from_numpy(scaler.transform(S_t).astype(np.float32))
        self.S_tp1 = torch.from_numpy(scaler.transform(S_tp1).astype(np.float32))
        self.A_t   = torch.from_numpy(A.astype(np.float32))

    def __len__(self):
        return self.S_t.shape[0]

    def __getitem__(self, idx):
        return {"S_t": self.S_t[idx], "A_t": self.A_t[idx], "S_tp1": self.S_tp1[idx]}

def fit_state_scaler_on_csv(
        csv_path: str, state_cols: List[str] = STATE_COLS_DEFAULT
    ) -> StandardScaler:
    """
    Fit a stateful StandardScaler on the training set only (consistent with FNN convention).
    """
    df = pd.read_csv(csv_path)
    assert all(c in df.columns for c in state_cols), "训练集缺少状态列；请检查 CSV 与列名"
    scaler = StandardScaler()
    scaler.fit(df[state_cols].astype(np.float32).values)
    return scaler

# --- S1-2: Saving Tools ---
def _save_json(obj, path: Path):
    path.parent.mkdir(parents=True, exist_ok=True)
    with open(path, "w", encoding="utf-8") as f:
        json.dump(obj, f, ensure_ascii=False, indent=2)

def _save_scaler_npz(scaler: StandardScaler, path: Path):
    path.parent.mkdir(parents=True, exist_ok=True)
    np.savez(path, mean=scaler.mean_.astype(np.float32), scale=scaler.scale_.astype(np.float32))

"""## **Model Construction: *GNN Transfer Dynamics***"""

# -----------------------------
# Model Consturction: GNN Transfer Dynamics
# -----------------------------

class NodeMLP(nn.Module):
    def __init__(self, in_dim, hid_dim, out_dim, num_layers=2, act=nn.SiLU, use_spectral_norm=False):
        super().__init__()
        layers = []
        d = in_dim
        for _ in range(num_layers - 1):
            linear_layer = nn.Linear(d, hid_dim)
            # S5 (Lipschitz): Apply Lipschitz spectral normalization
            if use_spectral_norm:
                linear_layer = spectral_norm(linear_layer)
            layers += [linear_layer, act()]
            d = hid_dim

        output_layer = nn.Linear(d, out_dim)
        # S5 (Lipschitz): Apply Lipschitz spectral normalization
        if use_spectral_norm:
            output_layer = spectral_norm(output_layer)
        layers += [output_layer]
        self.net = nn.Sequential(*layers)

    def forward(self, x):
        return self.net(x)

class GNNTransition(nn.Module):
    """
    [S4 Revision] Message Passing Networks, supporting GCN, GAT, and RGCN layers:
    - GATConv implements dynamic, state-dependent edge weights
    - RGCNConv implements heterogeneous graph mechanisms
    """
    def __init__(
            self, action_dim: int, hid_dim: int = 64, layers: int = 3,
            use_pressure_residual: bool = False,
            # --- S4 params ---
            gnn_layer_type: str = "gcn",
            gnn_attention_heads: int = 4,
            gnn_num_relations: int = 5,
            # --- S5 param ---
            use_spectral_norm: bool = False
        ):
        super().__init__()
        self.action_dim = action_dim
        self.hid_dim = hid_dim
        self.layers = layers
        self.use_pressure_residual = use_pressure_residual
        self.gnn_layer_type = gnn_layer_type

        self.type_emb = nn.Embedding(3, hid_dim // 2)

        # S5 (Lipschitz): Spectral normalization is applied to all linear mapping layers to improve robustness and generalization.
        val_proj_linear = nn.Linear(1, hid_dim // 2)
        action_proj_linear = nn.Linear(action_dim, hid_dim)
        self.val_proj = spectral_norm(val_proj_linear) if use_spectral_norm else val_proj_linear
        self.action_proj = spectral_norm(action_proj_linear) if use_spectral_norm else action_proj_linear

        self.convs = nn.ModuleList()
        for _ in range(layers):
            if gnn_layer_type == "gcn":
                self.convs.append(GCNConv(hid_dim, hid_dim))
            elif gnn_layer_type == "gat":
                if hid_dim % gnn_attention_heads != 0:
                    raise ValueError("GAT: hid_dim must be divisible by gnn_attention_heads")
                self.convs.append(GATConv(hid_dim, hid_dim // gnn_attention_heads, heads=gnn_attention_heads))
            elif gnn_layer_type == "rgcn":
                self.convs.append(RGCNConv(hid_dim, hid_dim, num_relations=gnn_num_relations))
            else:
                raise ValueError(f"Unknown gnn_layer_type: {gnn_layer_type}")

        # Update the input of the MLP to "h_prev (hid_dim) + h_conv (hid_dim) + a_emb (hid_dim)"
        self.upd = nn.ModuleList([NodeMLP(in_dim=hid_dim * 3, hid_dim=hid_dim, out_dim=hid_dim, use_spectral_norm=use_spectral_norm) for _ in range(layers)])

        out_head_linear = nn.Linear(hid_dim, 1)
        self.out_head = spectral_norm(out_head_linear) if use_spectral_norm else out_head_linear

        # --- S5: Cox Risk Prediction Head ---
        # Learn a mapping from a graph representation of the current state to the risk of a "coherence horizon".
        self.risk_head = NodeMLP(hid_dim, hid_dim // 2, 1, num_layers=2, use_spectral_norm=use_spectral_norm)

    def forward(self, S_t: torch.Tensor, A_t: torch.Tensor, edge_index: Optional[torch.Tensor] = None, edge_type: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:
        B = S_t.shape[0]; dS = S_t.shape[1]
        device = S_t.device
        a_emb = torch.tanh(self.action_proj(A_t))

        node_types = torch.tensor(
            [0]*3 + [1]*3 + [2]*3 + [3],
            device=device
        )[:dS]
        type_e = self.type_emb(node_types).unsqueeze(0).expand(B, -1, -1)
        val_e = self.val_proj(S_t.view(B, dS, 1))
        h = torch.cat([type_e, val_e], dim=-1)

        for l in range(self.layers):
            h_prev_layer = h

            # Reshape for PyG layers: [B, 10, D] -> [B*10, D]
            h_flat = h.view(B * dS, self.hid_dim)

            # Copy edge_index for each graph in the batch
            edge_index_batched = torch.cat([edge_index.to(device) + i * dS for i in range(B)], dim=1)

            if self.gnn_layer_type == "gcn" or self.gnn_layer_type == "gat":
                h_conv_flat = self.convs[l](h_flat, edge_index_batched)
            elif self.gnn_layer_type == "rgcn":
                edge_type_batched = edge_type.to(device).repeat(B)
                h_conv_flat = self.convs[l](h_flat, edge_index_batched, edge_type_batched)

            h_conv = h_conv_flat.view(B, dS, self.hid_dim)

            a_rep = a_emb.unsqueeze(1).expand(-1, dS, -1)

            # Update: h_new = MLP(h_old, h_conv, a_emb)
            h_updated = self.upd[l](torch.cat([h_prev_layer, h_conv, a_rep], dim=-1))
            h = F.silu(h_updated)

        out = self.out_head(h).view(B, dS)

        # S5 (Cox): Compute risk
        # Global pooling (averaging) → risk head → log risk
        graph_embedding = h.mean(dim=1)
        log_risk = self.risk_head(graph_embedding).squeeze(-1)

        if self.use_pressure_residual:
            out_res = S_t.clone()
            out_res[:, 3:6] = S_t[:, 3:6] + out[:, 3:6]
            out_res[:, 0:3] = out[:, 0:3]; out_res[:, 6:9] = out[:, 6:9]
            return out_res, log_risk

        return out, log_risk

"""## **Training & Evaluations**"""

# -----------------------------
# Training & Evaluations
# -----------------------------

def compute_metrics(
        model, loader, device, V_fn, rho: float,
        phi_crit: Optional[float],
        rollout_H: int,
        action_search: str, num_delta_dirs: int, action_delta: float,
        softmin_tau: float,
        args,
        edge_index: Optional[torch.Tensor] = None,
        edge_type: Optional[torch.Tensor] = None
    ) -> Dict[str, float]:
    """
    [S5 Revision] Completely refactored evaluation function.
    1. Dynamically collects all S1-S5 metrics to ensure scalability.
    2. Calculates validation loss (Cox, CBF) exactly as the training logic.
    3. Removed the old, inconsistent CBF violation calculation method.
    4. Provides deeper insights into the stability of both teacher and optimal strategies.
    """
    model.eval()

    # metrics_collectors: Dynamically collect major indicators of S3-S5
    metrics_collectors = defaultdict(list)
    all_nstep_dvs = [] # Collect dV traces for all batches
    # Ljung-Box: Requires sequence residuals, aggregated by seq_id (skip if none)
    residuals_by_seq: Dict[int, List[float]] = {}

    with torch.no_grad():
        for batch in loader:
            batch = to_device(batch, device)
            S_t = batch["S_t"]
            A_t = batch["A_t"]
            S_tp1 = batch["S_tp1"]

            S_pred, log_risk = model(S_t, A_t, edge_index, edge_type)

            # Basic Metrics (RMSE, W1)
            metrics_collectors["rmse"].append(rmse(S_pred, S_tp1))
            metrics_collectors["w1"].append(wasserstein1_vector(S_pred, S_tp1))
            # --- S1-4: Block Metrics（E/Phi/H）---
            E_pred, Phi_pred, H_pred = split_state(S_pred)
            E_true, Phi_true, H_true = split_state(S_tp1)
            metrics_collectors["rmse_E"].append(rmse(E_pred, E_true))
            metrics_collectors["rmse_Phi"].append(rmse(Phi_pred, Phi_true))
            metrics_collectors["rmse_H"].append(rmse(H_pred, H_true))
            metrics_collectors["w1_E"].append(wasserstein1_vector(E_pred, E_true))
            metrics_collectors["w1_Phi"].append(wasserstein1_vector(Phi_pred, Phi_true))
            metrics_collectors["w1_H"].append(wasserstein1_vector(H_pred, H_true))

            metrics_collectors["lyapunov_pos_rate_mean"].append(lyapunov_positive_rate(model, S_t, A_t, V_fn, edge_index=edge_index, edge_type=edge_type))

            # --- Stability/Safety under teacher's movements ---
            V_t = V_fn(S_t)
            V_next = V_fn(S_pred)
            tv = (V_next - (1.0 - rho) * V_t > 0).float().mean().item()
            metrics_collectors["clf_violation_rate_teacher"].append(tv)

            # Cox Loss (validation set)
            if args.lambda_cox > 0:
                T, E = batch["T"], batch["E"]
                loss_cox_val = cox_ph_loss(log_risk, T, E)
                metrics_collectors["cox_loss_val"].append(loss_cox_val.item())

            # CBF Loss (validation set, teacher action)
            if args.lambda_cbf > 0 and phi_crit is not None and phi_crit > 0:
                cbf_loss_teacher = control_barrier_loss(S_t, S_pred, phi_crit, args.cbf_alpha)
                metrics_collectors["cbf_loss_teacher_val"].append(cbf_loss_teacher.item())

            # CLF Violation Rate
            # Evaluation version: the loss is realized as training state, exploring the stability/security under the optimal action search
            # Only drift is evaluated and not included in loss weight
            _clf_loss, _clf_violation = clf_loss(
                model, S_t, A_t, V_fn, rho=rho,
                action_search=action_search,
                num_delta_dirs=num_delta_dirs,
                action_delta=action_delta,
                softmin_tau=softmin_tau,
                edge_index=edge_index, edge_type=edge_type,
                # S5: Under evaluation mode, only care about drift instead of penalty
                lambda_cbf=0.0, phi_crit=0.0, cbf_alpha=0.0
            )
            metrics_collectors["clf_violation_rate"].append(_clf_violation)

            # --- S3: N-Step CLF & Risk-Sensitive Metrics ---
            if args.use_nstep_clf:
                _, nstep_stats = n_step_clf_loss(
                    model=model, S_t=S_t, A_t=A_t, V_fn=V_fn,
                    nstep_H=args.nstep_H, nstep_gamma=1.0, # no discounts in eval
                    nstep_selector='hard_greedy', # use greedy in eval
                    nstep_bptt_window=-1,
                    use_cvar_loss=False, cvar_alpha=0.0, # no cvar in eval
                    use_epsilon_greedy=False, epsilon=0.0, # no explore in eval
                    policy_entropy_weight=0.0,
                    rho=rho, action_search=action_search,
                    num_delta_dirs=num_delta_dirs, action_delta=action_delta,
                    gumbel_tau=0.01, # harden
                    return_dv_trajectory=True, # request to return dV trace
                    edge_index=edge_index, edge_type=edge_type,
                    # S5: Under evaluation mode, only care about drift instead of penalty
                    lambda_adt=0.0, lambda_cbf=0.0,
                    phi_crit=0.0, cbf_alpha=0.0
                )
                for k, v in nstep_stats.items():
                    if k == "dv_trajectory":
                        all_nstep_dvs.append(v.cpu()) # collect dV traces
                    else:
                        metrics_collectors[k].append(v)

            # Multi-Step Negative Drift Coverage
            neg_cov = multi_step_negative_drift_coverage(
                model, S_t, A_t, V_fn, rho=rho, rollout_H=rollout_H,
                action_search=action_search,
                action_delta=action_delta,
                num_delta_dirs=num_delta_dirs,
                softmin_tau=softmin_tau,
                edge_index=edge_index, edge_type=edge_type
            )
            if not math.isnan(neg_cov):
                metrics_collectors["multi_step_neg_drift_coverage"].append(neg_cov)
            # Comparison 01: teacher-only rollout coverage
            cov_teacher = multi_step_negative_drift_coverage_teacher(
                model, S_t, A_t, V_fn, rho=rho, rollout_H=rollout_H,
                edge_index=edge_index, edge_type=edge_type
            )
            if not math.isnan(cov_teacher):
                metrics_collectors["multi_step_neg_drift_coverage_teacher"].append(cov_teacher)
            # Comparison 02: fixed-onehot(e_0) rollout coverage
            cov_fixed0 = multi_step_negative_drift_coverage_fixed(
                model, S_t, V_fn, rho=rho, rollout_H=rollout_H,
                fixed_idx=0, action_dim=A_t.shape[1],
                edge_index=edge_index, edge_type=edge_type
            )
            if not math.isnan(cov_fixed0):
                metrics_collectors["multi_step_neg_drift_coverage_fixed0"].append(cov_fixed0)

            # Residual collection (by sequence)
            if "seq_id" in batch:
                resid = (S_pred - S_tp1).mean(dim=1).detach().cpu().numpy()
                ids = batch["seq_id"].detach().cpu().numpy().tolist()
                for r, sid in zip(resid, ids):
                    residuals_by_seq.setdefault(int(sid), []).append(float(r))

    # Aggregate all metrics
    metrics = {}
    for key, val_list in metrics_collectors.items():
        if key.startswith("clf_violation_rate_step"):
            metrics[f"nstep_{key}"] = float(np.mean(val_list)) if val_list else float("nan")
            continue
        if key == "dv_trajectory": continue
        if key == "clf_violation_rate_mean":
            metrics[f"nstep_{key}"] = float(np.mean(val_list)) if val_list else float("nan")
        if val_list:
            metrics[key] = float(np.mean([v for v in val_list if not (v is None or math.isnan(v))]))
        else:
            metrics[key] = float("nan")

    # Ljung-Box Q
    # Calculate by sequence and then take the mean
    Q_list = []
    for sid, rseq in residuals_by_seq.items():
        Q_list.append(ljung_box_Q(np.array(rseq), lags=10))
    metrics["ljung_box_Q"] = float(np.nanmean(Q_list)) if len(Q_list) > 0 else float("nan")

    # Jacobian spectral norm (sampling estimate)
    # To control the cost, only run the gradient estimate again for the first batch of validation data (approximate)
    try:
        batch = next(iter(loader))
        batch = to_device(batch, device)
        metrics["jacobian_spectral_norm_mean"] = spectral_norm_jacobian_mean(
            model, batch["S_t"], batch["A_t"],
            num_power_iter=8, sample_size=64,
            edge_index=edge_index, edge_type=edge_type
        )
    except Exception:
        metrics["jacobian_spectral_norm_mean"] = float("nan")

    if len(all_nstep_dvs) > 0:
        all_nstep_dvs_tensor = torch.cat(all_nstep_dvs, dim=0) # [N_val, H]

        # Endpoint violation rate
        metrics["nstep_endpoint_violation_rate"] = (all_nstep_dvs_tensor[:, -1] > 0).float().mean().item()

        # Risk Sensitive Metrics (CVaR)
        # Care about the worst case scenario in all steps
        all_violations = F.relu(all_nstep_dvs_tensor.flatten())

        # Only count non-zero violations
        non_zero_violations = all_violations[all_violations > 1e-6]

        if non_zero_violations.numel() > 0:
            metrics["nstep_cvar_violation"] = cvar_loss(non_zero_violations.float(), args.cvar_alpha).item()
        else:
            metrics["nstep_cvar_violation"] = 0.0

    return metrics

def train_one_epoch(
        model, model_for_jac, loader, optimizer, device, grad_scaler,
        V_fn, args, epoch: int,
        edge_index: Optional[torch.Tensor] = None, edge_type: Optional[torch.Tensor] = None
    ) -> Dict[str, float]:
    model.train()

    mse_losses, clf_losses, total_losses, cox_losses = [], [], [], []
    clf_violation_rates = []

    # S3: Dynamically calculate the scheduling parameters of the current epoch
    rho_ep = _lin_schedule(epoch, args.epochs, args.rho, args.rho_final, args.rho_warmup_ep)
    lclf_ep = _lin_schedule(epoch, args.epochs, args.lambda_clf, args.lambda_clf_final, args.lambda_warmup_ep)
    gumbel_tau_ep = _exp_anneal(epoch, args.epochs, args.gumbel_tau_init, args.gumbel_tau_final, args.gumbel_anneal_ep)
    epsilon_ep = _lin_schedule(epoch, args.epochs, args.epsilon_init, args.epsilon_final, args.epsilon_decay_ep)

    for batch in loader:
        batch = to_device(batch, device)
        S_t = batch["S_t"]
        A_t = batch["A_t"]
        S_tp1 = batch["S_tp1"]
        # S5 (Cox): Get event data
        T = batch["T"]
        E = batch["E"]

        # Correction weight for supervision (imitation) loss: positive drift/high stress segments reduce supervision weight
        with torch.no_grad():
            # Calculate ΔV_true based on the teacher's true transfer
            V_t_true = V_fn(S_t)
            V_tp1_true = V_fn(S_tp1)
            dV_true = V_tp1_true - V_t_true  # [B]
            E_t, Phi_t, _ = split_state(S_t)
            # Simplified weights: w = 1 for dV <= 0; w = teacher_reweight_alpha for dV > 0 or high stress
            high_pressure = (torch.norm(Phi_t, dim=1) > 1.0).float()
            w = torch.ones_like(dV_true)
            w = torch.where(dV_true > 0, torch.full_like(w, args.teacher_reweight_alpha), w)
            w = torch.where(high_pressure > 0, torch.full_like(w, args.teacher_reweight_alpha), w)
            w = w.detach()

        with torch.amp.autocast(device_type=args.device, enabled=((args.device == 'cuda') and not sys.platform.startswith('win'))):

            # Predictions
            S_pred, log_risk = model(S_t, A_t, edge_index, edge_type)
            # Supervision loss (weighted)
            mse = torch.mean(w * torch.mean((S_pred - S_tp1) ** 2, dim=1))

            # S5 (Cox): Compute Cox Loss
            loss_cox = torch.tensor(0.0, device=device)
            if args.lambda_cox > 0:
                loss_cox = cox_ph_loss(log_risk, T, E)
                cox_losses.append(loss_cox.detach().item())

            # CLF Loss
            if args.use_nstep_clf:
                nstep_lambda = args.nstep_lambda if args.nstep_lambda is not None else lclf_ep
                clf_l, clf_stats = n_step_clf_loss(
                    model=model, S_t=S_t, A_t=A_t, V_fn=V_fn,
                    nstep_H=args.nstep_H, nstep_gamma=args.nstep_gamma,
                    nstep_selector=args.nstep_selector, nstep_bptt_window=args.nstep_bptt_window,
                    use_cvar_loss=args.use_cvar_loss, cvar_alpha=args.cvar_alpha,
                    use_epsilon_greedy=args.use_epsilon_greedy, epsilon=epsilon_ep,
                    policy_entropy_weight=args.policy_entropy_weight,
                    rho=rho_ep, action_search=args.action_search,
                    num_delta_dirs=args.num_delta_dirs, action_delta=args.action_delta,
                    gumbel_tau=gumbel_tau_ep,
                    edge_index=edge_index, edge_type=edge_type,
                    # S5 params
                    lambda_adt=args.lambda_adt, lambda_cbf=args.lambda_cbf,
                    phi_crit=args.phi_crit, cbf_alpha=args.cbf_alpha
                )
                clf_l = nstep_lambda * clf_l
                clf_violation = clf_stats["clf_violation_rate_mean"]
            else: # Degenerates to a single-step CLF
                clf_l, clf_violation = clf_loss(
                    model, S_t, A_t, V_fn, rho=rho_ep,
                    action_search=args.action_search, num_delta_dirs=args.num_delta_dirs,
                    action_delta=args.action_delta, softmin_tau=args.softmin_tau,
                    edge_index=edge_index, edge_type=edge_type,
                    # S5 params
                    lambda_cbf=args.lambda_cbf,
                    phi_crit=args.phi_crit, cbf_alpha=args.cbf_alpha
                )
                clf_l = lclf_ep * clf_l

            clf_violation_rates.append(clf_violation)

            # Jacobian Regularization: suppressing expansion mapping
            jac_pen = 0.0
            if args.jacobian_reg > 0.0:

                # Low-cost approximation: random subset + 2nd power iteration
                with torch.enable_grad():
                    sub = torch.randperm(S_t.shape[0], device=device)[:min(64, S_t.shape[0])]
                    S_sub = S_t[sub].detach().clone().requires_grad_(True)
                    A_sub = A_t[sub].detach().clone()
                    out, _ = model_for_jac(S_sub, A_sub, edge_index, edge_type)
                    v = torch.randn_like(out)
                    v = v / (torch.norm(v, dim=1, keepdim=True) + 1e-12)
                    JTv = None
                    for _ in range(2):
                        (JTv,) = torch.autograd.grad(outputs=out, inputs=S_sub, grad_outputs=v, retain_graph=True, create_graph=True)
                        sigma = torch.norm(JTv, dim=1, keepdim=True) + 1e-12
                        v = JTv / sigma
                    jac_sigma = torch.norm(JTv, dim=1).mean()
                    jac_pen = args.jacobian_reg * jac_sigma

            # Total Loss
            loss = mse + clf_l + jac_pen + (args.lambda_cox * loss_cox)

        # grad_scaler automatically scales the loss to prevent vanishing gradients,
        # adjusts the gradients accordingly, and then updates the model weights when safe.
        optimizer.zero_grad()
        grad_scaler.scale(loss).backward()
        # Gradient clipping
        grad_scaler.unscale_(optimizer)
        nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)

        grad_scaler.step(optimizer)
        grad_scaler.update()

        mse_losses.append(mse.detach().item())
        clf_losses.append(clf_l.detach().item())
        total_losses.append(loss.detach().item())

    return {
        "loss": float(np.mean(total_losses)),
        "mse": float(np.mean(mse_losses)),
        "clf_loss": float(np.mean(clf_losses)),
        "clf_violation_rate": float(np.mean(clf_violation_rates)) if clf_violation_rates else float("nan"),
        "cox_loss": float(np.mean(cox_losses)) if cox_losses else float("nan")
    }

# --- S2: Simple Scheduler ---
def _lin_schedule(ep, total_ep, start, end, warmup_ep):
    if end is None or warmup_ep <= 0:
        return start
    t = min(max(ep / max(1, warmup_ep), 0.0), 1.0)
    return float(start + (end - start) * t)

def _exp_anneal(ep, total_ep, start, end, anneal_ep):
    if end is None or anneal_ep <= 0:
        return start
    r = min(max(ep / max(1, anneal_ep), 0.0), 1.0)
    # Exponential Annealing: from start to end
    return float(start * (end / max(1e-12, start)) ** r)

def _pick_action_curriculum(ep, p):
    # p: dict Include segment settings
    if ep <= p["ep_switch1"]:
        return p["p1"]
    elif ep <= p["ep_switch2"]:
        return p["p2"]
    else:
        return p["p3"]

# --- S2: Validation set export: optimal action statistics and rolling trajectory ---
def _argmin_softmin_deltaV(
        model, S, A_teacher, V_fn,
        rho, cand,
        edge_index: Optional[torch.Tensor] = None, edge_type: Optional[torch.Tensor] = None
    ):
    # Returns: best_action_idx [B], best_dV [B], best_Snext [B,9]
    K, B, dA = cand.shape
    with torch.no_grad():
        V_now = V_fn(S) # [B]
        best_dv = None
        best_Sn = None
        best_k = torch.zeros(B, dtype=torch.long, device=S.device)
        for k in range(K):
            S_next, _ = model(S, cand[k], edge_index, edge_type) # [B,9]
            V_next = V_fn(S_next)
            dv = V_next - (1.0 - rho) * V_now
            if best_dv is None:
                best_dv, best_Sn = dv, S_next
                best_k[:] = k
            else:
                mask = dv < best_dv
                best_k = torch.where(mask, torch.full_like(best_k, k), best_k)
                best_dv = torch.where(mask, dv, best_dv)
                best_Sn = torch.where(mask.view(-1,1), S_next, best_Sn)
        # Convert best_k to a one-hot action index (for one-hot candidates, k is the action index;
        # for perturb/both, the action may not be purely one-hot, so "candidate index k" is returned here for statistical purposes).
        return best_k, best_dv, best_Sn

def export_val_action_stats_and_rollout(
        model, loader, device, V_fn, rho,
        action_search, num_delta_dirs, action_delta,
        save_dir,
        edge_index: Optional[torch.Tensor] = None, edge_type: Optional[torch.Tensor] = None,
        export_stats=False, export_rollout=False,
    ):
    if not (export_stats or export_rollout):
        return
    model.eval()
    all_k = []
    rollout_rows = []  # (seq_id, step, best_k, V, dV)

    with torch.no_grad():
        for batch in loader:
            batch = to_device(batch, device)
            S = batch["S_t"]
            A = batch["A_t"]
            seq_ids = batch.get("seq_id", None)

            cand = build_action_candidates(
                S_t=S, A_t=A,
                mode=action_search,
                num_delta_dirs=num_delta_dirs,
                action_delta=action_delta
            )   # [K,B,dA]

            best_k, best_dV, S_next = _argmin_softmin_deltaV(model, S, A, V_fn, rho, cand, edge_index, edge_type)
            if export_stats:
                all_k.append(best_k.detach().cpu().numpy())

            if export_rollout:
                V_now = V_fn(S)
                # A single-step trajectory is sufficient (multiple-step trajectories will be larger;
                # if multiple steps are required, you can imitate the multi-step implementation of compute_metrics)
                for i in range(S.shape[0]):
                    sid = int(seq_ids[i].item()) if seq_ids is not None else -1
                    rollout_rows.append({
                        "seq_id": sid,
                        "step": 0,
                        "best_k": int(best_k[i].item()),
                        "V_t": float(V_now[i].item()),
                        "dV": float(best_dV[i].item())
                    })

    save_dir = Path(save_dir); save_dir.mkdir(parents=True, exist_ok=True)
    if export_stats and len(all_k):
        import numpy as np, json
        ks = np.concatenate(all_k, axis=0)
        hist = {}
        note_out = "k is the candidate index (equal to the action index in case of onehot)"
        for int_k in ks.tolist():
            hist[int_k] = hist.get(int_k, 0) + 1
        with open(save_dir / "val_best_action_hist.json", "w", encoding="utf-8") as f:
            json.dump({"hist": hist, "note": note_out}, f, ensure_ascii=False, indent=2)

    if export_rollout and len(rollout_rows):
        import pandas as pd
        pd.DataFrame(rollout_rows).to_csv(save_dir / "val_rollout_one_step.csv", index=False)

"""## **Main Function**"""

# -----------------------------
# Main Function
# -----------------------------

def main():
    parser = argparse.ArgumentParser()

    # Dataset
    parser.add_argument("--train_path", type=str, default="./strategy_comparison_stepwise.csv")
    parser.add_argument("--val_path", type=str, default="")
    parser.add_argument("--val_ratio", type=float, default=0.1, help="When val_path is empty or invalid, the ratio of the validation set to the training set")
    parser.add_argument("--seed", type=int, default=42)
    parser.add_argument("--format", type=str, default="csv", choices=["csv", "npz"])
    parser.add_argument("--s_prefix", type=str, default="S_t_")
    parser.add_argument("--sp1_prefix", type=str, default="S_tp1_")
    parser.add_argument("--a_prefix", type=str, default="A_t_")
    parser.add_argument("--batch_size", type=int, default=512)
    parser.add_argument("--num_workers", type=int, default=0)

    # Models
    parser.add_argument("--hid_dim", type=int, default=64)
    parser.add_argument("--layers", type=int, default=3)
    parser.add_argument("--adj_mode", type=str, default="tri", choices=["tri", "full", "chain"])
    parser.add_argument("--use_pressure_residual", action="store_true")

    # --- S4: GNN Architecture Upgrade ---
    parser.add_argument("--gnn_layer_type", type=str, default="gcn", choices=["gcn", "gat", "rgcn"], help="Type of GNN layer: 'gcn' (baseline), 'gat' (graph attention network), 'rgcn' (relational graph network)")
    parser.add_argument("--gnn_attention_heads", type=int, default=4, help="The number of attention heads used by GATConv (must be divisible by hid_dim)")
    parser.add_argument("--gnn_num_relations", type=int, default=4, help="Number of relation types used by RGCNConv")
    parser.add_argument("--relation_mapping", type=str, default="auto", choices=["auto", "group_internal", "functional"], help="Strategy for relation type mapping")

    # Training
    parser.add_argument("--epochs", type=int, default=20)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--weight_decay", type=float, default=1e-4)
    parser.add_argument("--device", type=str, default="cuda" if torch.cuda.is_available() else "cpu")
    parser.add_argument("--save_dir", type=str, default="./runs/gnn_S1", help="Output directory (checkpoints/logs/config/Scaler)")

    # CLF/Stability
    parser.add_argument("--rho", type=float, default=0.2)
    parser.add_argument("--lambda_clf", type=float, default=2.0)
    parser.add_argument("--jacobian_reg", type=float, default=1e-3)
    parser.add_argument("--teacher_reweight_alpha", type=float, default=0.5)
    parser.add_argument("--softmin_tau", type=float, default=0.5)
    parser.add_argument("--rollout_H", type=int, default=12)

    parser.add_argument("--action_search", type=str, choices=["onehot", "perturb", "both"], default="onehot", help="Candidate action search mode: onehot=discrete one-hot, perturb=teacher action ± perturbation, both=merge both")
    parser.add_argument("--num_delta_dirs", type=int, default=2, help="The number of directions of the teacher action ± perturbation (<= dA), only valid when action_search=perturb/both")
    parser.add_argument("--action_delta", type=float, default=0.25, help="The step size of the teacher action ± perturbation, only valid when action_search=perturb/both")

    # CBF
    parser.add_argument("--phi_crit", type=float, default=0.0, help="If it is >0, the CBF violation rate is counted; if it is 0/negative, it is not counted.")

    # Diagonal weights of V(E) (learnable or fixed)
    parser.add_argument("--learn_P_diag", action="store_true", help="If set, then learn V(E)=E^T P E in the P diagonal.")
    parser.add_argument("--P_init", type=float, nargs=3, default=[1.0, 1.0, 1.0], help="Initial value of the diagonal of V(E)")

    # --- S2: Schedule scheduling ---
    parser.add_argument("--rho_final", type=float, default=None, help="If given, linearly increase rho from the current --rho to rho_final (over rho_warmup_ep rounds).")
    parser.add_argument("--rho_warmup_ep", type=int, default=10)

    parser.add_argument("--lambda_clf_final", type=float, default=None, help="If given, linearly upgrade lambda_clf from the current --lambda_clf to lambda_clf_final (after lambda_warmup_ep rounds)")
    parser.add_argument("--lambda_warmup_ep", type=int, default=12)

    parser.add_argument("--softmin_tau_final", type=float, default=None, help="If given, exponentially anneal softmin_tau from its current value to tau_final (in softmin_anneal_ep rounds).")
    parser.add_argument("--softmin_anneal_ep", type=int, default=12)

    # Candidate action set curriculum:
    # 3 segments, can be freely cut into 1~3 segments; closed by default
    parser.add_argument("--action_curriculum", action="store_true", help="Enable staged refinement of candidate action sets")
    parser.add_argument("--ep_switch1", type=int, default=5)
    parser.add_argument("--ep_switch2", type=int, default=12)

    parser.add_argument("--phase1_action", type=str, default="onehot", choices=["onehot","perturb","both"])
    parser.add_argument("--phase1_dirs", type=int, default=0)
    parser.add_argument("--phase1_delta", type=float, default=0.25)

    parser.add_argument("--phase2_action", type=str, default="both", choices=["onehot","perturb","both"])
    parser.add_argument("--phase2_dirs", type=int, default=2)
    parser.add_argument("--phase2_delta", type=float, default=0.25)

    parser.add_argument("--phase3_action", type=str, default="both", choices=["onehot","perturb","both"])
    parser.add_argument("--phase3_dirs", type=int, default=5)
    parser.add_argument("--phase3_delta", type=float, default=0.10)

    # --- S2: Stability-first model selection ---
    parser.add_argument("--earlystop_mode", type=str, default="rmse", choices=["rmse","stability_first"], help="S2: 'stability_first' minimizes lyapunov_pos_rate_mean and clf_violation_rate first, then looks at rmse")
    parser.add_argument("--stab_lexi_eps", type=float, default=1e-4, help="S2: Lexicographic tolerance of stability indices")

    # --- S2: Evaluation export (action frequency and scrolling trajectory) ---
    parser.add_argument("--export_action_stats", action="store_true", help="Validation set export: optimal action index of each sample and overall histogram")
    parser.add_argument("--export_rollout_csv", action="store_true", help="Validation set export: (seq_id, step, V, ΔV, action index) track rolling by best action迹")

    # --- S3: N-Step CLF Core Parameters ---
    parser.add_argument("--use_nstep_clf", action="store_true", help="Enable N-step CLF loss instead of single-step CLF")
    parser.add_argument("--nstep_H", type=int, default=3, help="Rolling step size H of N-step CLF")
    parser.add_argument("--nstep_gamma", type=float, default=0.98, help="N-step CLF loss future step discount factor gammaa")
    parser.add_argument("--nstep_lambda", type=float, default=1.0, help="N-step CLF loss weight (if None, reuse lambda_clf)")
    parser.add_argument("--nstep_aggr", type=str, default="stepwise", choices=["endpoint", "stepwise"], help="Aggregation method for N-step CLF targets: 'endpoint' or 'stepwise'")
    parser.add_argument("--nstep_bptt_window", type=int, default=-1, help="N-step CLF backpropagation cutoff window. -1 means full horizon (H), 1 means only current step.")

    # --- S3: Selector Annealing & Straight-Through Estimator ---
    parser.add_argument("--nstep_selector", type=str, default="softmin", choices=["softmin", "gumbel_st", "hard_greedy"], help="Action selector for each step in an N-step CLF")
    parser.add_argument("--gumbel_tau_init", type=float, default=1.0, help="Initial temperature of Gumbel-Softmax")
    parser.add_argument("--gumbel_tau_final", type=float, default=0.1, help="Final temperature of Gumbel-Softmax")
    parser.add_argument("--gumbel_anneal_ep", type=int, default=15, help="Number of annealing rounds for Gumbel-Softmax")

    # --- S3: Quantile/Risk-Sensitive Objective ---
    parser.add_argument("--use_cvar_loss", action="store_true", help="Use CVaR loss instead of mean() for N-step CLF violations")
    parser.add_argument("--cvar_alpha", type=float, default=0.8, help="CVaR quantile, e.g., 0.8 means penalizing the worst 20%")

    # --- S3: Candidate Diversity & Exploration ---
    parser.add_argument("--use_epsilon_greedy", action="store_true", help="Enabling epsilon-greedy exploration in action selection in N-step CLF")
    parser.add_argument("--epsilon_init", type=float, default=0.3, help="epsilon-greedy initial exploration rate")
    parser.add_argument("--epsilon_final", type=float, default=0.01, help="Final exploration rate of epsilon-greedy")
    parser.add_argument("--epsilon_decay_ep", type=int, default=15, help="Decay round number of epsilon")
    parser.add_argument("--policy_entropy_weight", type=float, default=0.0, help="The weight of the policy entropy regularization term, used to encourage action diversity (e.g., 0.01)")

    # --- S5 arguments ---
    # CBF
    parser.add_argument("--lambda_cbf", type=float, default=0.0, help="Weight of CBF loss (>0 to enable)")
    parser.add_argument("--cbf_alpha", type=float, default=0.5, help="CBF's Class-K function coefficient alpha")
    # ADT
    parser.add_argument("--lambda_adt", type=float, default=0.0, help="Weight of ADT (Action Switching) loss (>0 to enable)")
    # Cox
    parser.add_argument("--lambda_cox", type=float, default=0.0, help="Cox PH risk loss weight (>0 to enable)")
    parser.add_argument("--cox_event_threshold", type=float, default=2.0, help="Defines the error_norm threshold for \"coherence horizon\" events")
    # Lipschitz
    parser.add_argument("--use_spectral_norm", action="store_true", help="Apply spectral normalization to all linear layers in the model")

    args, _unknown = parser.parse_known_args()
    _set_rand_seed(args.seed) # The entire process is reproducible
    rng = np.random.default_rng(args.seed)
    print(f'args.learn_P_diag: {args.learn_P_diag}')

    # --- S1-3: Unified save directory and configuration ---
    save_dir = Path(args.save_dir)
    save_dir.mkdir(parents=True, exist_ok=True)
    # Save Configurations
    _save_json(vars(args), save_dir / "config.json")
    # Prepare JSONL training log path
    log_jsonl_path = save_dir / "metrics.jsonl"

    # Read CSV (default ./strategy_comparison_stepwise.csv)
    if not os.path.exists(args.train_path):
        raise FileNotFoundError(f"Invalid training set path: {args.train_path}")
    train_df = pd.read_csv(args.train_path)

    # Constructing triples
    S_t_all, A_all, S_tp1_all, seq_id_all, T_all, E_all = _build_pairs_from_df(train_df, action_dim=5, event_threshold=args.cox_event_threshold)

    # Split train/val
    if args.val_path and os.path.exists(args.val_path):
        # Using an independent validation set
        val_df = pd.read_csv(args.val_path)
        S_t_tr, A_tr, S_tp1_tr, seq_tr, T_tr, E_tr = S_t_all, A_all, S_tp1_all, seq_id_all, T_all, E_all
        S_t_va, A_va, S_tp1_va, seq_va, T_va, E_va = _build_pairs_from_df(val_df, action_dim=5, event_threshold=args.cox_event_threshold)
    else:
        # Split from the training set
        N = S_t_all.shape[0]
        idx = rng.permutation(N)
        n_val = max(1, int(N * args.val_ratio))
        val_idx, tr_idx = idx[:n_val], idx[n_val:]
        S_t_tr, A_tr, S_tp1_tr, seq_tr, T_tr, E_tr = S_t_all[tr_idx], A_all[tr_idx], S_tp1_all[tr_idx], seq_id_all[tr_idx], T_all[tr_idx], E_all[tr_idx]
        S_t_va, A_va, S_tp1_va, seq_va, T_va, E_va = S_t_all[val_idx], A_all[val_idx], S_tp1_all[val_idx], seq_id_all[val_idx], T_all[val_idx], E_all[val_idx]

    # Fit StandardScaler only on the training set (same as FNN)
    state_scaler = StandardScaler().fit(S_t_tr.astype(np.float32))
    _save_scaler_npz(state_scaler, save_dir / "scaler.npz") # Save scaler to save_dir

    # Building a PyTorch Dataset/DataLoader
    train_ds = ArrayTripletDataset(S_t_tr, A_tr, S_tp1_tr, state_scaler, seq_id=seq_tr, T=T_tr, E=E_tr)
    val_ds = ArrayTripletDataset(S_t_va, A_va, S_tp1_va, state_scaler, seq_id=seq_va, T=T_va, E=E_va)

    train_loader = DataLoader(train_ds, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers, drop_last=False)
    val_loader   = DataLoader(val_ds, batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers, drop_last=False)

    # Inferring action dimensions
    action_dim = train_ds.A_t.shape[1]

    # Model and Optimizer
    # [S4 Update] Generating Graph Structure
    adj, edge_index, edge_type = build_adjacency(args.adj_mode, args.gnn_num_relations, args.relation_mapping)

    # Models and Optimizers
    model = GNNTransition(
        action_dim=action_dim,
        hid_dim=args.hid_dim,
        layers=args.layers,
        use_pressure_residual=args.use_pressure_residual,
        # --- S4 params ---
        gnn_layer_type=args.gnn_layer_type,
        gnn_attention_heads=args.gnn_attention_heads,
        gnn_num_relations=args.gnn_num_relations,
        use_spectral_norm=args.use_spectral_norm
    ).to(args.device)

    # Keep an uncompiled model reference specifically
    # for calculations requiring second-order gradients.
    model_for_jac = model

    if sys.version_info >= (3, 8) and torch.__version__ >= "2.0.0":
        print("PyTorch 2.0+ detected, Compiling the model...")
        model = torch.compile(model)
        print("Model compilation successful!")

    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    grad_scaler = torch.amp.GradScaler('cuda', enabled=(args.device == 'cuda'))

    # Implementation of V(E) = E^T P E (P diagonally positive definite)
    P_diag_param = nn.Parameter()
    P_init = torch.tensor(args.P_init, dtype=torch.float32, device=args.device)
    if args.learn_P_diag:
        # Use softplus to keep positive
        P_diag_param = nn.Parameter(torch.log(torch.exp(P_init) - 1.0 + 1e-6))  # softplus^{-1}(P_init)
        optimizer.add_param_group({"params": [P_diag_param], "lr": args.lr})
        def V_fn_from_S(S: torch.Tensor) -> torch.Tensor:
            E, _, _ = split_state(S)
            P_diag = F.softplus(P_diag_param) + 1e-6
            return lyapunov_V(E, P_diag)
        V_fn = V_fn_from_S
    else:
        P_diag_fixed = P_init.detach()
        def V_fn_from_S(S: torch.Tensor) -> torch.Tensor:
            E, _, _ = split_state(S)
            return lyapunov_V(E, P_diag_fixed)
        V_fn = V_fn_from_S

    # Training Loop
    best_val_rmse = float("inf")
    val_metrics = {}

    # --- S2: curriculum combination (only enabled when action_curriculum=True) ---
    if args.action_curriculum:
        curr_cfg = {
            "ep_switch1": args.ep_switch1,
            "ep_switch2": args.ep_switch2,
            "p1": (args.phase1_action, args.phase1_dirs, args.phase1_delta),
            "p2": (args.phase2_action, args.phase2_dirs, args.phase2_delta),
            "p3": (args.phase3_action, args.phase3_dirs, args.phase3_delta),
        }
    best_pack = {"rmse": float("inf"), "lyap": float("inf"), "clf_v": float("inf")}
    for epoch in range(1, args.epochs + 1):

        # --- S2: Super parameters after this round of scheduling ---
        rho_ep  = _lin_schedule(epoch, args.epochs, args.rho, args.rho_final, args.rho_warmup_ep)
        lclf_ep = _lin_schedule(epoch, args.epochs, args.lambda_clf, args.lambda_clf_final, args.lambda_warmup_ep)
        tau_ep  = _exp_anneal(epoch, args.epochs, args.softmin_tau, args.softmin_tau_final, args.softmin_anneal_ep)

        if args.action_curriculum:
            a_mode, a_dirs, a_delta = _pick_action_curriculum(epoch, curr_cfg)
        else:
            a_mode, a_dirs, a_delta = args.action_search, args.num_delta_dirs, args.action_delta

        train_log = train_one_epoch(
            model, model_for_jac, train_loader, optimizer, args.device, grad_scaler,
            V_fn=V_fn, args=args, epoch=epoch,
            edge_index=edge_index, edge_type=edge_type
        )

        val_metrics = compute_metrics(
            model, val_loader, args.device, V_fn=V_fn, rho=rho_ep,
            phi_crit=(args.phi_crit if args.phi_crit > 0 else None),
            rollout_H=args.rollout_H,
            action_search=a_mode,
            num_delta_dirs=a_dirs,
            action_delta=a_delta,
            softmin_tau=tau_ep,
            args=args,
            edge_index=edge_index, edge_type=edge_type
        )

        # --- S2: Early stopping/selection with stability as the priority ---
        improved = False
        if args.earlystop_mode == "rmse":
            improved = (val_metrics["rmse"] < best_val_rmse)
        else:
            # Lexicographic order:
            # 1. minimize lyapunov_pos_rate_mean,
            # 2. minimize clf_violation_rate,
            # 3. minimize rmse
            eps = args.stab_lexi_eps
            lyap, clf_v, r = val_metrics["lyapunov_pos_rate_mean"], val_metrics["clf_violation_rate"], val_metrics["rmse"]
            if (lyap + eps < best_pack["lyap"] or
            (abs(lyap - best_pack["lyap"]) <= eps and (clf_v + eps < best_pack["clf_v"] or
                (abs(clf_v - best_pack["clf_v"]) <= eps and r < best_pack["rmse"])))):
                improved = True

        if improved:
            best_val_rmse = val_metrics["rmse"]
            best_pack["rmse"] = val_metrics["rmse"]
            best_pack["lyap"] = val_metrics["lyapunov_pos_rate_mean"]
            best_pack["clf_v"] = val_metrics["clf_violation_rate"]
            # S1: Save the best checkpoint
            if args.learn_P_diag:
                P_diag_curr = (F.softplus(P_diag_param).detach().cpu().numpy() + 1e-6).tolist()
            else:
                P_diag_curr = P_init.detach().cpu().numpy().tolist()
            torch.save({
                "model": model.state_dict(),
                "optimizer": optimizer.state_dict(),
                "config": vars(args),
                "best_val_rmse": best_val_rmse,
                "scaler_mean": state_scaler.mean_.astype(np.float32),
                "scaler_scale": state_scaler.scale_.astype(np.float32),
                "P_diag": P_diag_curr
            }, (save_dir / "gnn_checkpoint_best.pt"))

        # --- S1-3: Record the status of this round of scheduling (append to JSONL) ---
        # S2: Updated to rho_ep, lclf_ep, tau_ep, a_mode, a_dirs, a_delta
        if args.learn_P_diag:
            P_diag_log = (F.softplus(P_diag_param).detach().cpu().numpy() + 1e-6).tolist()
        else:
            P_diag_log = P_init.detach().cpu().numpy().tolist()
        log = {
            "epoch": epoch,
            "train": train_log,
            "val": val_metrics,
            "P_diag": P_diag_log,
            "sched": {
                "rho": rho_ep, "lambda_clf": lclf_ep, "softmin_tau": tau_ep,
                "action_search": a_mode, "num_delta_dirs": a_dirs, "action_delta": a_delta
            },
            "save_dir": str(save_dir)
        }
        print(json.dumps(log, ensure_ascii=False))
        with open(log_jsonl_path, "a", encoding="utf-8") as f:
            f.write(json.dumps(log, ensure_ascii=False) + "\n")

    # After training, export the final model
    if args.learn_P_diag:
        P_diag_curr = (F.softplus(P_diag_param).detach().cpu().numpy() + 1e-6).tolist()
    else:
        P_diag_curr = P_init.detach().cpu().numpy().tolist()
    torch.save({
        "model": model.state_dict(),
        "optimizer": optimizer.state_dict(),
        "config": vars(args),
        "best_val_rmse": best_val_rmse,
        "scaler_mean": state_scaler.mean_.astype(np.float32),
        "scaler_scale": state_scaler.scale_.astype(np.float32),
        "P_diag": P_diag_curr
    }, (save_dir / "gnn_checkpoint_last.pt"))

    # --- S1-3: Export the final verification indicator ---
    _save_json(val_metrics, save_dir / "metrics_last.json")

    # S2: After the training cycle is completed (after saving last),
    # export the validation set statistics based on the current model
    export_val_action_stats_and_rollout(
        model, val_loader, args.device, V_fn, rho=best_pack["lyap"] if args.earlystop_mode=="stability_first" else args.rho,
        action_search=args.action_search if not args.action_curriculum else curr_cfg["p3"][0],
        num_delta_dirs=args.num_delta_dirs if not args.action_curriculum else curr_cfg["p3"][1],
        action_delta=args.action_delta if not args.action_curriculum else curr_cfg["p3"][2],
        save_dir=save_dir,
        export_stats=args.export_action_stats,
        export_rollout=args.export_rollout_csv,
        edge_index=edge_index, edge_type=edge_type
    )

if __name__ == "__main__":
    main()